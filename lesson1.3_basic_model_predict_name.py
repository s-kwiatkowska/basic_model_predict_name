#!/usr/bin/env python
# coding: utf-8

# # Predykcja pÅ‚ci po imieniu
# 
# ### Celem jest zrobienie prostej, ale juÅ¼ wartoÅ›ciowej predykcji.
# 
# * [pandas](https://bit.ly/3sy04Jw) - biblioteka do wczytania i manipulacji danymi
# * [numpy](https://bit.ly/2Pe9A65) - biblioteka do pracy z wektorami/macierzami, pandas wewnÄ…trz rÃ³wnieÅ¼ uÅ¼ywa `numpy`
# * [sklearn](https://bit.ly/3fzXLlF) - biblioteka, ktÃ³ra zawiera konkretne implementacje algorytmÃ³w uczenia maszynowego (wymawia siÄ™ *[saÉª-kit-lÉ™:n]*, to jest skrÃ³cona wersja od `"science-kit-learn"`)

# ### Krok po kroku 
# 
# JeÅ›li wolisz najpierw sÅ‚uchaÄ‡ i oglÄ…daÄ‡, to obejrzyj nagranie poniÅ¼ej, ktÃ³re omawia tÄ™ lekcjÄ™. 

# In[1]:


get_ipython().run_cell_magic('html', '', '<iframe style="height:500px;width:100%" src="https://bit.ly/3swlfLZ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>')


# In[15]:


import pandas as pd

#modele (algorytmy)
from sklearn.dummy import DummyClassifier           # <== Najprostszy moÅ¼liwy model 
from sklearn.linear_model import LogisticRegression # <== Regresja logistyczna (liniowa)

#metryka sukcesu
from sklearn.metrics import accuracy_score


# **Uwaga!** nazwa moduÅ‚u (*LogisticRegression*) wskazuje, Å¼e jest to regresja logistyczna, natomiast to jest podklasa regresji liniowej (czyli pod spodem jest zwykÅ‚a regresja liniowa + dodatkowa funkcja na koÅ„cu).

# ## Wczytujemy dane
# 
# Dane sÄ… w formacie `.csv`, `pandas` umoÅ¼liwia w jednym wierszu wczytanie danych w formacie `.csv`: `.read_csv()`. Po uruchomieniu tej linii `df` bÄ™dzie zawieraÄ‡ dane wczytane z pliku w postaci tabelarycznej (czyli wiersze i kolumny).

# In[16]:


df = pd.read_csv("../input/polish_names.csv")
df.head()


# ## Sprawdzamy dane
# 
# Na poczÄ…tek chcemy wiedzieÄ‡ bardzo proste rzeczy:
# 1. Ile jest wierszy (wszystkich obiektÃ³w)?
# 2. Ile jest kolumn (cech obiektÃ³w)?
# 3. KtÃ³ra zmienna jest zmiennÄ… docelowÄ… (ang. *target variable*)?
# 4. Jaki problem jest do rozwiÄ…zania (klasyfikacja czy regresja)?
# 5. W przypadku klasyfikacji, ile (dwie czy wiÄ™cej) i jakie unikalne wartoÅ›ci ma zmienna docelowa?
# 6. Jak wyglÄ…da rozkÅ‚ad unikalnych wartoÅ›ci zmiennej docelowej (czy jest mniej wiÄ™cej po rÃ³wno, czy jednak sÄ… bardzo popularne/rzadkie klasy)?
# 7. Czy sÄ… brakujÄ…ce dane?

# In[17]:


df.info()


# - Druga linia "mÃ³wi": `1705 entries`, to jest iloÅ›Ä‡ wierszy (obiektÃ³w).
# - Trzecia linia "mÃ³wi": `total 2 columns`, co oznacza, Å¼e mamy 2 kolumny (cechy).
# - NastÄ™pnie mamy informacjÄ™ o kaÅ¼dej kolumnie i liczbÄ™ wartoÅ›ci (`non-null`). 
# - JeÅ›li kolumna X ma mniej `non-null` wierszy niÅ¼ caÅ‚oÅ›Ä‡, to oznacza, Å¼e dla tej cechy mamy brakujÄ…ce wartoÅ›ci (ang. *missing data*), z ktÃ³rymi trzeba bÄ™dzie sobie "jakoÅ›" poradziÄ‡.
# - W naszym przypadku (na poczÄ…tku) wszystko jest bardzo proste. Mamy wszystkie wartoÅ›ci i tylko jednÄ… cechÄ™ - **imiÄ™**. A druga kolumna to jest zmienna docelowa (eng. *target variable*), czyli czy imiÄ™ jest **mÄ™skie** czy **Å¼eÅ„skie** (tylko dwie wartoÅ›ci, wiÄ™c klasyfikacja binarna). 
# - Ostatnia linia `memory usage` mÃ³wi, ile pamiÄ™ci RAM zuÅ¼ywa, w tym przypadku bardzo maÅ‚o (jedynie 26.7 KB).

# ## Jak wyglÄ…dajÄ… dane?
# Zobacz 10 losowych wierszy.

# In[18]:


df.sample(10)


# - Kolumna `name` zawiera imiÄ™ i czasem sÄ… doÅ›Ä‡ ciekawe :).
# - Kolumna `gender` zawiera pÅ‚eÄ‡, gdzie **`m`** oznacza imiÄ™ mÄ™skie a **`f`** - imiÄ™ Å¼eÅ„skie
# 
# SprawdÅºmy, jaki jest rozkÅ‚ad imion **mÄ™skich** i **Å¼eÅ„skich**.

# In[19]:


df['gender'].value_counts()


# - MÄ™skich imion jest prawie 2 razy wiÄ™cej (**1033** do **672**).
# - Dalej bÄ™dzie widaÄ‡, czy jest to dla nas jakiÅ› problem (np. przez to, Å¼e imion Å¼eÅ„skich jest mniej, jakoÅ›Ä‡ modelu jest gorsza. JeÅ›li tak, to bÄ™dziemy pÃ³Åºniej myÅ›leÄ‡, co z tym zrobiÄ‡).
# 
# PamiÄ™tasz, Å¼e model oczekuje na reprezentacjÄ™ liczbowÄ… zamiast sÅ‚ownÄ…? Teraz mamy transformowaÄ‡: `m => 1, f => 0`.
# 
# PomoÅ¼e nam w tym funkcja [`map`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html). Å»eby lepiej zrozumieÄ‡, jak dziaÅ‚a funkcja .map(), zrÃ³bmy to w kilku krokach.
# 
# Funkcja `transform_string_into_number` zwraca to samo, co dostaÅ‚a, to tak zwana funkcja [funkcja toÅ¼samoÅ›ciowa](https://pl.wikipedia.org/wiki/Funkcja_to%C5%BCsamo%C5%9Bciowa). Robimy to po to, Å¼eby poznaÄ‡ skÅ‚adniÄ™.

# In[20]:


def transform_string_into_number(string):
    return string
    
df['gender'].head().map( transform_string_into_number )


# Teraz dodajmy logikÄ™ do funkcji `transform_string_into_number`

# In[21]:


def transform_string_into_number(string):
    return int(string == 'm')
    
df['gender'].head().map( transform_string_into_number )


# UÅ¼yjmy teraz anonimowej funkcji (*lambda*), Å¼eby zmniejszyÄ‡ iloÅ›Ä‡ kodu. Wynik mapowania przypisujemy do nowej kolumny o nazwie `target`.
# 
# *ZwrÃ³Ä‡ uwagÄ™*, Å¼e *lambda* nie ma sÅ‚owa kluczowego `return`, bo to z definicji ma byÄ‡ jednowierszowa logika (wynik, ktÃ³ry zostanie zwrÃ³cony).

# In[22]:


df['target'] = df['gender'].map( lambda x: int(x == 'm') )
df.head(10)


# ## Feature engineering
# Dodajmy pierwszÄ… cechÄ™, np. dÅ‚ugoÅ›Ä‡ imienia. ZaÅ‚Ã³Å¼my, Å¼e iloÅ›Ä‡ literek moÅ¼e wpÅ‚ynÄ…Ä‡ na predykcjÄ™, czy imiÄ™ jest mÄ™skie czy Å¼eÅ„skie.
# 
# Dlaczego akurat tak? Od czegoÅ› musimy zaczÄ…Ä‡ i to jest jedna z prostszych cech, ktÃ³rÄ… moÅ¼na wnioskowaÄ‡ na podstawie sÅ‚owa. Czy jest skuteczna? WÅ‚aÅ›nie to chcemy sprawdziÄ‡.
# 
# ![](../images/len_fi.png)

# ## Zadanie 1.3.1
# Twoim zadaniem jest stworzyÄ‡ nowÄ… cechÄ™ (kolumnÄ™), ktÃ³ra bÄ™dzie zawieraÄ‡ dÅ‚ugoÅ›Ä‡ imienia (moÅ¼esz stworzyÄ‡ wiÄ™cej niÅ¼ jednÄ… cechÄ™, o ile masz na to pomysÅ‚y).
# 
# 

# In[23]:


import pandas as pd

df = pd.read_csv("../input/polish_names.csv")
df.head()

df['len_name'] = df['name'].map( lambda x:len(x))

df.head(10)


# <details>
#     <summary style="background: #e6eaeb; padding: 4px 0; text-align: center; font-size: 20px; font-weight: 900;"> ğŸ‘‰ Kliknij tutaj (1 klik), aby zobaczyÄ‡ podpowiedÅº ğŸ‘ˆ </summary>
# <p>
# DÅ‚ugoÅ›Ä‡ w Python mierzy siÄ™ przy pomocy funkcji len, np. len("Abc").
# <details>
#     <summary style="background: #e6eaeb; padding: 4px 0; text-align: center; font-size: 20px; font-weight: 900;"> ğŸ‘‰ Kliknij tutaj (1 klik), aby zobaczyÄ‡ odpowied ğŸ‘ˆ </summary>
# <p>
# 
# ```python
# df['len_name'] = df['name'].map(lambda x: len(x))
# ```
# 
# </p>
# </details>
# </p>
# </details> 

# ### ğŸ¤ğŸ—£ï¸ WspÃ³Å‚praca ğŸ’ª i komunikacja ğŸ’¬
# 
# - ğŸ‘‰ [#pml_module1](https://practicalmlcourse.slack.com/archives/C045CNLNH89) - to jest miejsce, gdzie moÅ¼na szukaÄ‡ pomocy i dzieliÄ‡ siÄ™ doÅ›wiadczeniem - takÅ¼e pomagaÄ‡ innym ğŸ¥°. 
# 
# JeÅ›li masz pytanie, to staraj siÄ™ jak najdokÅ‚adniej je sprecyzowaÄ‡, najlepiej wrzuÄ‡ screen z twoim kodem i bÅ‚Ä™dem, ktÃ³ry siÄ™ pojawiÅ‚ âœ”ï¸
# 
# - ğŸ‘‰ [#pml_module1_done](https://practicalmlcourse.slack.com/archives/C045CP89KND) - to miejsce, gdzie moÅ¼esz dzieliÄ‡ siÄ™ swoimi przerobionymi zadaniami, wystarczy, Å¼e wrzucisz screen z #done i numerem lekcji np. *#1.2.1_done*, Å›miaÅ‚o dodaj komentarz, jeÅ›li czujesz takÄ… potrzebÄ™, a takÅ¼e rozmawiaj z innymi o ich rozwiÄ…zaniach ğŸ˜Š 
# 
# - ğŸ‘‰ [#pml_module1_ideas](https://practicalmlcourse.slack.com/archives/C044TFZLF1U)- tutaj moÅ¼esz dzieliÄ‡ siÄ™ swoimi pomysÅ‚ami
# 

# ## Pierwszy model
# - ZrÃ³bmy nasz pierwszy model (eng. *basic model*), ktÃ³ry bÄ™dzie doÅ›Ä‡ prosty, wrÄ™cz "gÅ‚upi", zresztÄ… ma on takÄ… nazwÄ™ `DummyClassifier`.
# - Bardzo polecam zaczynaÄ‡ od czegoÅ› bardzo prostego, to pomoÅ¼e zrozumieÄ‡, gdzie jesteÅ› teraz i mieÄ‡ pierwszy wynik, z ktÃ³rym moÅ¼emy siÄ™ porÃ³wnywaÄ‡ (np. jeÅ›li pÃ³Åºniej spÄ™dzimy kilka tygodni robiÄ…c coÅ› bardziej zaawansowanego, a byÄ‡ moÅ¼e warto byÅ‚o zastosowaÄ‡ tylko najprostszy model?)
# - Idea polega na tym, Å¼e model patrzy tylko na zmiennÄ… docelowÄ…, jak czÄ™sto wystÄ™pujÄ… te czy inne klasy (w naszym przypadku imiÄ™ mÄ™skie lub Å¼eÅ„skie).
# 
# 
# ## Przygotujmy dane
# Metoda odpowiedzialna do trenowania modelu ma nazwÄ™: **`fit`** i oczekuje ona 2 argumentÃ³w:
# - Pierwszy argument to jest **macierz/tablica** cech (**Uwaga**: cecha moÅ¼e byÄ‡ jedna, ale to nadal ma byÄ‡ tablica, nie wektor!)
# - Drugi argument to **wektor** zmiennej docelowej (eng. *target variable*)
# 
# 
# **PodpowiedÅº**:
# - `[1, 2, 3, 4, 5]` => to jest wektor
# - `[[1], [2], [3], [4], [5]]` => to jest wektor wektorÃ³w, czyli macierz/tablica (w tym przypadku tylko z jednÄ… cechÄ™ dla kaÅ¼dego obiektu)
# - `[[1, 10], [2, 20], [3, 30], [4, 40], [5, 50]]` => to jest wektor wektorÃ³w, czyli macierz (w tym przypadku dwie cechy dla kaÅ¼dego obiektu)

# In[24]:


#na wszelki wypadek odpowiedÅº na zadanie 1.3.1
df['len_name'] = df['name'].map(lambda x: len(x))


# Teraz przygotujmy `X`, `y` i wytrenujmy pierwszy model.

# In[25]:


X = df[ ['len_name'] ].values
y = df['target'].values

model = DummyClassifier(strategy = 'stratified')
model.fit(X, y)
y_pred = model.predict(X)


# Jak juÅ¼ masz tablicÄ™ **X** (cechy dla naszych obiektÃ³w) i wektor **y** (odpowiedzi dla obiektÃ³w lub zmiennÄ… docelowÄ… [eng. *target variable*]), to juÅ¼ moÅ¼emy zaczÄ…Ä‡ budowaÄ‡ model.
# 
# Ten proces skÅ‚ada siÄ™ z prostych trzech krokÃ³w:
# 1. WybÃ³r modelu (algorytmu) i utworzenie instancji
# 2. Trenowanie modelu (podajÄ…c X i y) => **`fit(X_train, y_train)`**
# 3. Predykcja modelu (w tym przypadku podajemy tylko cechy, bo odpowiedÅº zwraca model) => **`predict(X_test)`**
# 
# *SwojÄ… drogÄ…*, zwykle odpowiedÅº z modelu jest przypisywana do zmiennej `y_pred` (oczywiÅ›cie moÅ¼esz tÄ… zmiennÄ… nazwaÄ‡, jak tylko chcesz), ale polecam trzymaÄ‡ siÄ™ tej konwencji.
# 
# Teraz moÅ¼emy przypisaÄ‡ `y_pred` do nowej kolumny i zobaczyÄ‡, ile przydzieliÅ‚ imion mÄ™skich, a ile Å¼eÅ„skich.

# In[26]:


df['gender_pred'] = y_pred
df['gender_pred'].value_counts()


# Zobaczmy teraz, w ilu przypadkach model podaÅ‚ innÄ… odpowiedÅº, niÅ¼ byÅ‚a w rzeczywistoÅ›ci.

# In[27]:


df[ df.target != y_pred ].shape # bÅ‚Ä™dna odpowiedÅº


# PamiÄ™taj, Å¼e `1` oznacza imiÄ™ mÄ™skie oraz `0` oznacza imiÄ™ Å¼eÅ„skie.
# 
# ZwrÃ³Ä‡ uwagÄ™, w ilu przypadkach (`df[ df.target != y_pred ].shape`) z 1705 model pomyliÅ‚ siÄ™. Model byÅ‚ o tyle "mÄ…dry", Å¼e jedynie uwzglÄ™dniÅ‚ Ã³wczesny rozkÅ‚ad (przypomnÄ™, Å¼e byÅ‚o 1033 vs 672) i to dlatego uznaÅ‚, Å¼e imiÄ™ mÄ™skie ma wystÄ™powaÄ‡ czÄ™Å›ciej. OczywiÅ›cie takie podejÅ›cie jest bÅ‚Ä™dne... ale juÅ¼ moÅ¼na wyciÄ…gnÄ…Ä‡ ciekawe wnioski o tym, jak Å‚atwo znieksztaÅ‚ciÄ‡ rzeczywistoÅ›Ä‡ modelu podajÄ…c pewne dane czÄ™Å›ciej lub rzadziej. 
# 
# *SwojÄ… drogÄ…* ciekawy [artykuÅ‚](https://medium.com/@angebassa/data-alone-isnt-ground-truth-9e733079dfd4) o tym, Å¼e dane nie sÄ… prawdÄ… absolutnÄ….
# 
# Kolejnym krokiem jest zmierzenie jakoÅ›ci. Dla uproszczenia bÄ™dziemy patrzeÄ‡ na `accuracy`, czyli dokÅ‚adnoÅ›Ä‡ naszego modelu (w tej chwili opuÅ›cimy inne moÅ¼liwe metryki, Å¼eby uproÅ›ciÄ‡ poczÄ…tek).

# In[28]:


accuracy_score(y, y_pred)


# Mamy ok. 50%, wynik jest bardzo bliski do losowego (50% zawsze moÅ¼emy osiÄ…gnÄ…Ä‡, po prostu podrzucajÄ…c monetÄ™).
# 
# **Uwaga!** `accuracy_score` sprawdza, jak wiele wartoÅ›ci dla wektora `y_pred` pokrywa siÄ™ z wektorem `y` i pokazuje wynik w procentach. WiÄ™cej o metrykach bÄ™dzie w nastÄ™pnym module.

# ### LosowoÅ›Ä‡
# `DummyClassifier` ignoruje cechy i zwraca zawsze ten sam wynik, o ile ustawisz `random_state`. JeÅ›li nie ustawisz `random_state`, to wynik za kaÅ¼dym razem bÄ™dzie siÄ™ trochÄ™ rÃ³Å¼niÅ‚ (moÅ¼esz to sprawdziÄ‡).

# In[29]:


model = DummyClassifier(strategy = 'stratified', random_state=0)
model.fit(X, y)
y_pred = model.predict(X)
accuracy_score(y, y_pred)


# ## Liniowy model
# 
# UÅ¼yjmy teraz modelu liniowego `LogisticRegression` (pamiÄ™taj, Å¼e regresja logistyczna to jest regresja liniowa + na koÅ„cu funkcja binarna, ktÃ³ra zwraca 0 lub 1).
# 
# Jest wiele parametrÃ³w, ktÃ³re moÅ¼na sprecyzowaÄ‡ dla modelu. W tym przypadku zdefiniujemy tylko `solver`, czyli algorytm, ktÃ³ry jest wykorzystywany do obliczania modelu. Na tym etapie nie ma duÅ¼ego znaczenia, ktÃ³ry wybierzemy, wiÄ™c uÅ¼yjemy domyÅ›lnego dla `LogisticRegression`.
# 
# *SwojÄ… drogÄ…* nazwa `LogisticRegression` jest doÅ›Ä‡ mylÄ…ca, bo sama nazwa wskazuje na robienie regresji, jednak w rzeczywistoÅ›ci robi siÄ™ klasyfikacjÄ™. SkÄ…d taka nazwa? Jak to czÄ™sto bywa w Å¼yciu, sÄ… na to pewne powody historyczne :D.
# 
# MoÅ¼e jeszcze raz powtÃ³rzÄ™, na wszelki wypadek `LogisticRegression` to jest liniowy model dla **klasyfikacji** (nie regresji). Nazwa jest jaka jest, warto zapamiÄ™taÄ‡ :).

# In[30]:


model = LogisticRegression(solver='lbfgs')
model.fit(X, y)
y_pred = model.predict(X)
accuracy_score(y, y_pred)


# Jak widaÄ‡, jakoÅ›Ä‡ modelu juÅ¼ jest lepsza. UdaÅ‚o siÄ™ nam osiÄ…gnÄ…Ä‡ **~61%** dokÅ‚adnoÅ›ci. SprawdÅºmy, jak wyglÄ…da rozkÅ‚ad odpowiedzi.

# In[31]:


df['gender_pred'] = y_pred
df['gender_pred'].value_counts()


# To oznacza, Å¼e model zawsze zwrÃ³ciÅ‚ `1` (kaÅ¼de imiÄ™ to imiÄ™ mÄ™skie), bo akurat ta klasa byÅ‚a bardziej popularna. ZrÃ³bmy eksperyment, jeÅ›li manualnie przypiszesz zawsze odpowiedÅº `1`,
# to dostaniesz ten sam wynik.

# In[32]:


y_pred = [1]*X.shape[0] #iloÅ›Ä‡ jedynek powinna zgadzaÄ‡ siÄ™ z iloÅ›ciÄ… wierszy w macierzy X
accuracy_score(y, y_pred)


# Dlaczego tak siÄ™ dzieje?
# 
# Na obecnych cechach model liniowy nie potrafiÅ‚ siÄ™ lepiej nauczyÄ‡ i uznaÅ‚, Å¼e takie podejÅ›cie jest najbardziej rozsÄ…dne. 
# 
# Dlaczego `accuracy` jest ok. 61% przy tak gÅ‚upim podejÅ›ciu? To wynika ze sÅ‚aboÅ›ci tej metryki, ktÃ³ra bardzo zaleÅ¼y od rozkÅ‚adu (wiÄ™cej o tym w drugim module).
# 
# Zaraz dodamy kolejnÄ… cechÄ™, ale juÅ¼ moÅ¼na zauwaÅ¼yÄ‡, Å¼e poprzednia komÃ³rka skÅ‚adajÄ…ca siÄ™ z 4 linijek kodu bÄ™dzie siÄ™ powtarzaÄ‡. To oznacza, Å¼e warto zrobiÄ‡ osobnÄ… funkcjÄ™, Å¼eby uÅ‚atwiÄ‡ sobie Å¼ycie w przyszÅ‚oÅ›ci. Niech to bÄ™dzie funkcja o nazwie: `train_and_predict_model`.

# In[33]:


def train_and_predict_model(X, y, model, success_metric=accuracy_score):
    model.fit(X, y)
    y_pred = model.predict(X)
    
    print("Distribution:")
    print( pd.Series(y_pred).value_counts() )
    
    return success_metric(y, y_pred)


# **Uwaga!** 
# MoÅ¼emy sobie wywoÅ‚aÄ‡ `success_metric(y, y_pred)`, co przy wczeÅ›niejszej deklaracji `success_metric=accuracy_score` oznacza, Å¼e `accuracy_score` dostanie te same parametry, ktÃ³re przekazaliÅ›my do `success_metric`. Python umoÅ¼liwia przekazywanie parametrÃ³w domyÅ›lnych do funkcji w taki sposÃ³b (co nie jest moÅ¼liwe np. w takich jÄ™zykach jak Java czy PHP, ale jest normalne dla wszystkich jÄ™zykÃ³w funkcyjnych).

# ## Cechy
# Popracujemy nad samogÅ‚oskami. ByÄ‡ moÅ¼e ich liczba i kolejnoÅ›Ä‡ wpÅ‚ywa na to, czy jest to imiÄ™ mÄ™skie czy Å¼eÅ„skie.

# In[34]:


vowels = ['a', 'Ä…', 'e', 'Ä™', 'i', 'o', 'u', 'y']

def how_many_vowels(name):
    count = sum( map(lambda x: int(x in vowels), name.lower()) )
    
    return count

#how_many_vowels('Jana')

df['count_vowels'] = df['name'].map(how_many_vowels)
train_and_predict_model(df[['len_name', 'count_vowels'] ], y, LogisticRegression(solver='lbfgs'))


# UdaÅ‚o siÄ™ polepszyÄ‡ wynik o 10 punktÃ³w procentowych! Bardzo dobrze,  prÃ³bujmy dalej. Nowa cecha bÄ™dzie sprawdzaÄ‡, czy pierwsza litera jest samogÅ‚oskÄ… czy nie.
# 
# ZwrÃ³Ä‡ uwagÄ™, Å¼e rozkÅ‚ad odpowiedzi juÅ¼ jest w miarÄ™ sensowny **1082** vs **623** (nie tylko same "1", czyli imiona mÄ™skie).

# In[35]:


def first_is_vowel(name):
    return name.lower()[0] in vowels

#first_is_vowel('Ada')

df['first_is_vowel'] = df['name'].map(first_is_vowel)

train_and_predict_model(df[['len_name', 'first_is_vowel'] ], y, LogisticRegression(solver='lbfgs'))


# Jak widaÄ‡, ta cecha w ogÃ³le nie wpÅ‚ynÄ™Å‚a na jakoÅ›Ä‡ modelu... To jest normalnie. Tak naprawdÄ™ doÅ›Ä‡ czÄ™sto bÄ™dziemy prÃ³bowaÄ‡ rÃ³Å¼nych pomysÅ‚Ã³w i wiÄ™kszoÅ›Ä‡ z nich moÅ¼e nie dziaÅ‚aÄ‡. Trzeba byÄ‡ na to przygotowanym i Å¼yÄ‡ wg zasady: `Fail fast, learn faster`. 
# 
# ZwrÃ³Ä‡ uwagÄ™, Å¼e tym razem model zwrÃ³ciÅ‚ tylko 1 (imiÄ™ mÄ™skie), czyli nie potrafiÅ‚ "wymyÅ›liÄ‡" nic lepszego. To oznacza, Å¼e cecha "czy pierwsza litera to samogÅ‚oska?" jest bezuÅ¼yteczna (dla modelu liniowego).
# 
# Idziemy dalej. SprawdÅºmy teraz razem trzy cechy: dÅ‚ugoÅ›Ä‡ imienia, iloÅ›Ä‡ samogÅ‚osek oraz czy pierwsza litera to samogÅ‚oska.

# In[36]:


X = df[['len_name', 'count_vowels', 'first_is_vowel'] ]
train_and_predict_model(X, y, LogisticRegression(solver='lbfgs'))


# UdaÅ‚o siÄ™ ulepszyÄ‡ model o kolejne **1.5%** (**0.714** vs **0.729**). Bardzo dobrze, idziemy dalej.
# 
# Tylko najpierw poznajmy lepiej funkcjÄ™ `.factorize()`.

# In[37]:


pd.factorize(['blue', 'green', 'yellow', 'blue'])


# Jak widzisz, `pd.factorize()` zwrÃ³ciÅ‚a tuple z dwoma wynikami.
# - pierwsze to sÄ… unikalne ID `array([0, 1, 2, 0])`
# - drugi to etykietki do ID'kÃ³w, zobacz `blue=0` lub `yellow=2` (czyli `yellow`  ma indeks dwa w tablice `['blue', 'green', 'yellow']`)
# 
# W naszym przypadku bÄ™dzie trzeba przekazaÄ‡ ID'ki dla modelu, czyli potrzebujemy tylko pierwszÄ… czÄ™Å›Ä‡ wyniku:
# `pd.factorize(['blue', 'green', 'yellow', 'blue'])[0]`. ZwrÃ³Ä‡ uwagÄ™, Å¼e na koÅ„cu pojawiÅ‚o siÄ™ `[0]`.

# In[38]:


pd.factorize(['blue', 'green', 'yellow', 'blue'])[0]


# FunkcjÄ™ `.factorize()` moÅ¼emy zrobiÄ‡ w taki sposÃ³b: `pd.factorize()` lub w taki `df['new_column'].factorize()` wynik dziaÅ‚ania bÄ™dzie identyczny, ale druga wersja czasem jest wygodniejsza w pisaniu.
# 
# WrÃ³Ä‡my do naszych cech, czyli przypiszmy kaÅ¼dej literce unikalny ID.

# ## Zadanie 1.3.2
# 
# Napisz podobny kod jak wyÅ¼ej, tylko wyciÄ…gnij ostatniÄ… literÄ™ jako cechÄ™ (zamiast pierwszej).

# In[47]:


df['last_letter'] = df['name'].map(lambda x: x.lower()[-1])
df['last_letter_cnt'] = df['last_letter'].factorize()[0]

X = df[['len_name','count_vowels','first_is_vowel','last_letter_cnt']]

train_and_predict_model(X, y, LogisticRegression(solver='lbfgs'))


# <details>
#     <summary style="background: #e6eaeb; padding: 4px 0; text-align: center; font-size: 20px; font-weight: 900;"> ğŸ‘‰ Kliknij tutaj (1 klik), aby zobaczyÄ‡ podpowiedÅº ğŸ‘ˆ </summary>
# <p>
# Musisz utworzyÄ‡ nowy atrybut last_letter, a nastÄ™pnie last_letter_cnt
# <details>
#     <summary style="background: #e6eaeb; padding: 4px 0; text-align: center; font-size: 20px; font-weight: 900;"> ğŸ‘‰ Kliknij tutaj (1 klik), aby zobaczyÄ‡ odpowiedÅº ğŸ‘ˆ </summary>
# <p>
# 
# ```python
# df['last_letter'] = df['name'].map(lambda x: x.lower()[-1])
# df['last_letter_cnt'] = df['last_letter'].factorize()[0]
# 
# X = df[['len_name', 'count_vowels', 'first_is_vowel', 'last_letter_cnt'] ]
# train_and_predict_model(X, y, LogisticRegression(solver='lbfgs'))
# ```
#  
# </p>
# </details>
# </p>
# </details> 

# ## Kolejne pomysÅ‚y na nowe cechy
# 1. Pobierzmy wszystkie samogÅ‚oski (ang. *vowels*), zakÅ‚adajÄ…c, Å¼e to moÅ¼e mieÄ‡ wpÅ‚yw. Na przykÅ‚ad **SÅ‚awomir** ma trzy samogÅ‚oski w tej kolejnoÅ›ci: **aoi**, natomiast **Patrycja** rÃ³wnieÅ¼ ma trzy samogÅ‚oski, ale inna kombinacja: **aya**. Dla kaÅ¼dej kombinacji pojawi siÄ™ unikalny ID.
# 2. ZrÃ³bmy podobnie, tylko tym razem spÃ³Å‚gÅ‚oski (ang. *consonants*).

# In[40]:


def get_all_vowels(name):
    all_vowels = [letter for letter in name.lower() if letter in vowels]
    
    return ''.join(all_vowels)
    
#get_all_vowels('SÅ‚awomir')

df['all_vowels'] = df['name'].map(get_all_vowels)
df['all_vowels_cnt'] = pd.factorize(df['all_vowels'])[0]


X = df[['len_name', 'count_vowels', 'first_is_vowel', 'first_letter_cnt', 'all_vowels_cnt'] ]
train_and_predict_model(X, y, LogisticRegression(solver='lbfgs'))


# In[41]:


def get_all_consonants(name):
    all_consonants = [letter for letter in name.lower() if letter not in vowels]
    
    return ''.join(all_consonants)
    
#get_all_consonants('SÅ‚awomir')

df['all_consonants'] = df['name'].map(get_all_consonants)
df['all_consonants_cnt'] = pd.factorize(df['all_consonants'])[0]

X = df[['len_name', 'count_vowels', 'first_is_vowel', 'first_letter_cnt', 'all_consonants_cnt'] ]
train_and_predict_model(X, y, LogisticRegression(solver='lbfgs', max_iter=200))


# TrochÄ™ lepiej (zwÅ‚aszcza pierwszy pomysÅ‚ z samogÅ‚oskami): **0.729** vs **0.738**. SpÃ³Å‚gÅ‚oski trochÄ™ poprawiÅ‚y model, ale mniej: **0.729** vs **0.731**. To raczej ma sens, prawda? SamogÅ‚oski majÄ… wiÄ™kszy wpÅ‚yw na to, czy imiÄ™ jest mÄ™skie czy Å¼eÅ„skie. 
# 
# KontynuujÄ…c myÅ›l. Kolejna cecha, ktÃ³ra moÅ¼e mieÄ‡ wpÅ‚yw, to jaka jest ostatnia litera. JeÅ›li jest to samogÅ‚oska, to raczej jest to imiÄ™ Å¼eÅ„skie, na przykÅ‚ad: **Kamila** i **Kamil**, **Adriana** i **Adrian** czy **Jana** i **Jan**. SprawdÅºmy to.

# In[42]:


def last_is_vowel(name):
    return name.lower()[-1] in vowels

#last_is_vowel('Ada')

df['last_is_vowel'] = df['name'].map(last_is_vowel)

X = df[['last_is_vowel'] ]
train_and_predict_model(X, y, LogisticRegression(solver='lbfgs', max_iter=200))


# Wow! Czy to widzisz? Tylko jedna cecha potrafi od razu daÄ‡ tak dobry wynik - **95%**. To dlatego proces `feature engineering` jest takim waÅ¼nym procesem. 
# 
# Musisz siÄ™ przyzwyczaiÄ‡, Å¼e najpierw trzeba siÄ™ namÄ™czyÄ‡, ale jest szansa, Å¼e wÅ‚aÅ›nie dziÄ™ki metodzie prÃ³b i bÅ‚Ä™dÃ³w wymyÅ›lisz bardzo sensownÄ… cechÄ™.
# 
# *SwojÄ… drogÄ…*, czy moÅ¼esz przypomnieÄ‡ sobie mÄ™skie imiÄ™, ktÃ³re koÅ„czy siÄ™ na "a"?

# In[43]:


feats = ['last_is_vowel', 'len_name', 'count_vowels', 'first_is_vowel', 'all_vowels_cnt', 'all_consonants_cnt']
X = df[ feats ]
train_and_predict_model(X, y, LogisticRegression(solver='lbfgs', max_iter=200))


# Pochwal siÄ™ na Slacku w kanale #pml_module1, Å¼e udaÅ‚o Ci siÄ™ przejÅ›Ä‡ przez lekcjÄ™ 1.3 i wytrenowaÄ‡ pierwszy model, Å›miaÅ‚o zaÅ‚Ä…cz screen z wynikiem :) 

# ### Ciekawostka
# SprawdÅºmy, jak czÄ™sto imiÄ™ mÄ™skie koÅ„czy siÄ™ na "a" i imiÄ™ Å¼eÅ„skie nie koÅ„czy siÄ™ na literkÄ™ "a".

# In[44]:


df.columns


# In[45]:


df['lst_letter_a'] = df.name.map(lambda x: x[-1] == 'a')

df[ (df.gender == 'm') & df.lst_letter_a ]


# Mamy 4 mÄ™skie imiona, ktÃ³re koÅ„czÄ… siÄ™ na literkÄ™ "a". 
# 
# *SwojÄ… drogÄ…*, jak pytaÅ‚em ludzi czy imiÄ™ Batszeba jest mÄ™skie czy Å¼eÅ„skie, to gÅ‚osy podzieliÅ‚y siÄ™ mniej wiÄ™cej pÃ³Å‚ na pÃ³Å‚ :). Jaka jest Twoja opinia? Czy Batszeba to imiÄ™ mÄ™skie czy Å¼eÅ„skie i dlaczego tak uwaÅ¼asz? 
# 
# SprawdÅºmy, ile Å¼eÅ„skich imion nie koÅ„czy siÄ™ na literkÄ™ "a".
# 
# *ZwrÃ³Ä‡ uwagÄ™*, Å¼e znak tyldy `~` negujÄ™ znak, to oznacza, Å¼e `(~df.lst_letter_a)` jest tym samym co `(False == df.lst_letter_a)`

# In[46]:


df[ (df.gender == 'f') & (~df.lst_letter_a) ]


# WÅ›rÃ³d tych 10 imion, ktÃ³re nie koÅ„czÄ… siÄ™ na "a", ile jest polskich? :)
# 
# MuszÄ™ Ci siÄ™ przyznaÄ‡, Å¼e ten wynik jest trochÄ™ optymistyczny. Dlaczego? To jest dobre pytanie. ZastanÃ³wmy siÄ™ nad tym.
# 
# **PamiÄ™taj**, Å¼e trenowanie modelu i weryfikowanie go na tych samych danych, to jest zÅ‚y pomysÅ‚. To jest tak samo, jak przyjdziesz na egzamin i wraz z pytaniami dostaniesz odpowiedzi :).
# 
# Ten efekt jest nazywany przeuczaniem siÄ™ (ang. [*`overfitting`*](https://en.wikipedia.org/wiki/Overfitting)) i sprawia doÅ›Ä‡ duÅ¼e problemy w uczeniu maszynowym. Trzeba nabraÄ‡ wprawy, Å¼eby z tym sobie radziÄ‡! 
# 
# Spokojnie damy radÄ™. JuÅ¼ w kolejnym Ä‡wiczeniu pokaÅ¼Ä™ Ci pierwszy sposÃ³b, jak sobie z tym radziÄ‡, a w kolejnym module spÄ™dzimy jeszcze wiÄ™cej czasu, Å¼eby to zrozumieÄ‡.
# 
# Tak jak powiedziaÅ‚em, to jest jedna z najwiÄ™kszych "bolÄ…czek" w uczeniu maszynowym, ktÃ³ra sprowadza siÄ™ do pytaÅ„: "Czy mogÄ™ zaufaÄ‡ modelowi? Czy ten model na pewno dziaÅ‚a (wystarczajÄ…co) dobrze?".

# ## Przydatne linki:
# * [Machine Learning 101](https://bit.ly/3lZUcGo)
# * [Machine Learning Glossary](https://bit.ly/3m280Ao)
# * [Data Alone Isnâ€™t Ground Truth](https://bit.ly/39o3I0Q)
# * [Numerical Optimization: Understanding L-BFGS](https://bit.ly/3w115f8)
# 

# In[ ]:




